{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c7d750-81ae-4c00-9617-cde941f002bf",
   "metadata": {},
   "source": [
    "## Superweight ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44276452-c672-4535-96f1-560c22a040d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbb2d4b-ea35-4b6a-b21a-87f33ca7dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"huggyllama/llama-7B\"\n",
    "DEVICE = \"cuda:0\"\n",
    "LIMIT = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94005777-4d26-4112-99bf-15d707aebbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUT = f\"outputs/llama7b_hellaswag_limit{LIMIT}_baseline_8bit.json\"\n",
    "ABL_OUT  = f\"outputs/llama7b_hellaswag_limit{LIMIT}_SW_ablation.json\"\n",
    "RAND_OUT = f\"outputs/llama7b_hellaswag_limit{LIMIT}_random.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b618bf3-43ab-4830-aebb-d7d715f6fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-26:15:24:32 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-02-26:15:24:38 INFO     [_cli.run:378] Selected Tasks: ['hellaswag']\n",
      "2026-02-26:15:24:38 INFO     [evaluator:213] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-02-26:15:24:38 INFO     [evaluator:238] Initializing hf-outlier model, with arguments: {'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True, 'outlier_method': 'manual_scaling_SO_0.0'}\n",
      "2026-02-26:15:24:38 INFO     [models.huggingface:161] Using device 'cuda:0'\n",
      "config.json: 100%|█████████████████████████████| 594/594 [00:00<00:00, 4.19MB/s]\n",
      "tokenizer_config.json: 2.28kB [00:00, 3.88MB/s]\n",
      "tokenizer.model: 100%|████████████████████████| 500k/500k [00:01<00:00, 448kB/s]\n",
      "tokenizer.json: 1.84MB [00:00, 19.6MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 411/411 [00:00<00:00, 3.24MB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "model.safetensors.index.json: 26.8kB [00:00, 57.4MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 5.29M/9.98G [00:10<5:36:44, 494kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|   | 5.29M/9.98G [00:23<5:36:44, 494kB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|  | 72.3M/9.98G [01:06<2:27:50, 1.12MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|   | 139M/9.98G [01:09<1:06:16, 2.47MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|     | 206M/9.98G [01:11<38:22, 4.24MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 268M/9.98G [01:15<27:40, 5.84MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 319M/9.98G [01:15<19:53, 8.09MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 386M/9.98G [01:21<17:20, 9.22MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 453M/9.98G [01:21<11:35, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 520M/9.98G [01:25<10:11, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 587M/9.98G [01:26<07:53, 19.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 654M/9.98G [01:27<06:01, 25.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 721M/9.98G [01:28<05:09, 29.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 788M/9.98G [01:29<04:04, 37.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 856M/9.98G [01:30<03:24, 44.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 923M/9.98G [01:32<03:52, 38.9MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 990M/9.98G [01:33<03:14, 46.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.06G/9.98G [01:33<02:31, 59.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▍   | 1.12G/9.98G [01:34<02:16, 65.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▍   | 1.19G/9.98G [01:36<03:04, 47.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.26G/9.98G [01:37<02:22, 61.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▌   | 1.32G/9.98G [01:38<02:28, 58.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▌   | 1.39G/9.98G [01:39<02:03, 69.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▌   | 1.46G/9.98G [01:39<01:40, 84.8MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 1.53G/9.98G [01:39<01:13, 114MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.59G/9.98G [01:40<01:13, 113MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.73G/9.98G [01:40<00:51, 159MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.86G/9.98G [01:40<00:36, 220MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.93G/9.98G [01:40<00:31, 254MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.06G/9.98G [01:41<00:30, 259MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.13G/9.98G [01:41<00:27, 288MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.20G/9.98G [01:41<00:24, 323MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.26G/9.98G [01:41<00:21, 359MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.33G/9.98G [01:42<00:20, 378MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.40G/9.98G [01:42<00:27, 274MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 2.53G/9.98G [01:42<00:18, 404MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.60G/9.98G [01:42<00:17, 431MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.67G/9.98G [01:42<00:20, 361MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.73G/9.98G [01:43<00:18, 399MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.80G/9.98G [01:43<00:16, 431MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.87G/9.98G [01:43<00:14, 474MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.93G/9.98G [01:43<00:16, 436MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 3.00G/9.98G [01:43<00:16, 412MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.07G/9.98G [01:43<00:17, 395MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.14G/9.98G [01:43<00:15, 428MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.27G/9.98G [01:44<00:12, 534MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.34G/9.98G [01:44<00:12, 534MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▋   | 3.47G/9.98G [01:44<00:09, 661MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.61G/9.98G [01:44<00:08, 789MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.74G/9.98G [01:44<00:08, 765MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.87G/9.98G [01:44<00:07, 800MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|██   | 4.01G/9.98G [01:44<00:06, 874MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.14G/9.98G [01:45<00:06, 897MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.28G/9.98G [01:45<00:06, 929MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.41G/9.98G [01:45<00:06, 888MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.54G/9.98G [01:45<00:05, 964MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.68G/9.98G [01:45<00:07, 673MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.81G/9.98G [01:46<00:07, 731MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▍  | 4.95G/9.98G [01:46<00:06, 788MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.08G/9.98G [01:46<00:05, 838MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.21G/9.98G [01:46<00:05, 913MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.35G/9.98G [01:46<00:05, 917MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▋  | 5.48G/9.98G [01:46<00:05, 771MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.62G/9.98G [01:47<00:06, 641MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.75G/9.98G [01:47<00:05, 714MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.89G/9.98G [01:47<00:05, 781MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 6.02G/9.98G [01:47<00:04, 846MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.15G/9.98G [01:47<00:04, 916MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.29G/9.98G [01:47<00:04, 853MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.42G/9.98G [01:48<00:05, 700MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.56G/9.98G [01:48<00:05, 621MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.69G/9.98G [01:48<00:04, 671MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.82G/9.98G [01:48<00:04, 760MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▍ | 6.96G/9.98G [01:48<00:03, 825MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.09G/9.98G [01:48<00:03, 914MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.23G/9.98G [01:48<00:02, 949MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.36G/9.98G [01:49<00:02, 971MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.50G/9.98G [01:49<00:02, 846MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.63G/9.98G [01:49<00:03, 658MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.76G/9.98G [01:49<00:03, 593MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.90G/9.98G [01:50<00:04, 489MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.96G/9.98G [01:50<00:04, 485MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.03G/9.98G [01:50<00:03, 510MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.10G/9.98G [01:50<00:03, 480MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.17G/9.98G [01:50<00:04, 433MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.23G/9.98G [01:51<00:03, 466MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.37G/9.98G [01:51<00:02, 584MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▎| 8.50G/9.98G [01:51<00:02, 659MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.64G/9.98G [01:51<00:01, 696MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.77G/9.98G [01:51<00:01, 770MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.90G/9.98G [01:51<00:01, 864MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.04G/9.98G [01:51<00:01, 935MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|███▋| 9.17G/9.98G [01:52<00:00, 1.01GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|███▋| 9.31G/9.98G [01:52<00:00, 1.05GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|███▊| 9.44G/9.98G [01:52<00:00, 1.08GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|███▊| 9.57G/9.98G [01:52<00:00, 1.11GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|███▉| 9.71G/9.98G [01:52<00:00, 1.12GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|███▉| 9.84G/9.98G [01:52<00:00, 1.15GB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|████| 9.98G/9.98G [01:52<00:00, 88.5MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████            | 1/2 [01:53<01:53, 113.03s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|    | 13.0M/3.50G [00:06<29:24, 1.98MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   0%|    | 13.0M/3.50G [00:16<29:24, 1.98MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 80.1M/3.50G [00:20<14:11, 4.02MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 147M/3.50G [00:25<08:17, 6.74MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 214M/3.50G [00:25<04:40, 11.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 281M/3.50G [00:26<02:54, 18.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 348M/3.50G [00:29<02:50, 18.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 482M/3.50G [00:30<01:22, 36.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 550M/3.50G [00:30<01:05, 45.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 617M/3.50G [00:30<00:47, 60.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▉    | 684M/3.50G [00:30<00:35, 80.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█▎    | 751M/3.50G [00:30<00:26, 103MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▌    | 885M/3.50G [00:31<00:15, 172MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 1.02G/3.50G [00:31<00:09, 258MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 1.15G/3.50G [00:31<00:06, 354MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 1.29G/3.50G [00:31<00:06, 325MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.42G/3.50G [00:32<00:05, 353MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.49G/3.50G [00:32<00:05, 348MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 1.62G/3.50G [00:32<00:04, 465MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▌  | 1.76G/3.50G [00:32<00:04, 434MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▋  | 1.89G/3.50G [00:32<00:03, 517MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 2.02G/3.50G [00:33<00:02, 599MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 2.16G/3.50G [00:33<00:02, 589MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|███▎ | 2.29G/3.50G [00:33<00:02, 571MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|███▍ | 2.43G/3.50G [00:33<00:01, 617MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|███▋ | 2.56G/3.50G [00:33<00:01, 684MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███▊ | 2.70G/3.50G [00:34<00:01, 754MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  81%|████ | 2.83G/3.50G [00:34<00:00, 825MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  85%|████▏| 2.96G/3.50G [00:34<00:00, 911MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████▍| 3.10G/3.50G [00:34<00:00, 962MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|████▌| 3.23G/3.50G [00:34<00:00, 884MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|████▊| 3.37G/3.50G [00:34<00:00, 780MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|█████| 3.50G/3.50G [00:34<00:00, 100MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [02:28<00:00, 74.18s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:09<00:00,  4.69s/it]\n",
      "generation_config.json: 100%|██████████████████| 137/137 [00:00<00:00, 1.48MB/s]\n",
      "manual scaling SO\n",
      "Original SO: {(2, 3968, 7003): -127}\n",
      "Layer 2, Index [3968, 7003], Old value: -127, New value: -0.0\n",
      "Not restoring or scaling GO...\n",
      "2026-02-26:15:27:27 INFO     [evaluator_utils:446] Selected tasks:\n",
      "2026-02-26:15:27:27 INFO     [evaluator_utils:480] Task: hellaswag (hellaswag/hellaswag.yaml)\n",
      "2026-02-26:15:27:27 INFO     [api.task:312] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████████| 300/300 [00:00<00:00, 2431.46it/s]\n",
      "2026-02-26:15:27:27 INFO     [evaluator:575] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 1200/1200 [03:14<00:00,  6.15it/s]\n",
      "2026-02-26:15:30:44 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "hf-outlier ({'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True, 'outlier_method': 'manual_scaling_SO_0.0'}), gen_kwargs: ({}), limit: 300.0, num_fewshot: None, batch_size: 1\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |↑  |0.3400|±  |0.0274|\n",
      "|         |       |none  |     0|acc_norm|↑  |0.4267|±  |0.0286|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} evaluate.py \\\n",
    "  --model hf-outlier \\\n",
    "  --model_args pretrained={MODEL},load_in_8bit=True,outlier_method=manual_scaling_SO_0.0 \\\n",
    "  --tasks hellaswag \\\n",
    "  --device {DEVICE} \\\n",
    "  --limit {LIMIT} \\\n",
    "  --output_path {ABL_OUT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ee868-f35c-4ce0-9271-b2a0a7501133",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a4010f-5a7c-459a-a439-49fcd7515821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-26:15:31:47 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-02-26:15:31:55 INFO     [_cli.run:378] Selected Tasks: ['hellaswag']\n",
      "2026-02-26:15:31:55 INFO     [evaluator:213] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-02-26:15:31:55 INFO     [evaluator:238] Initializing hf-outlier model, with arguments: {'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True}\n",
      "2026-02-26:15:31:55 INFO     [models.huggingface:161] Using device 'cuda:0'\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.50s/it]\n",
      "Not restoring or scaling GO...\n",
      "2026-02-26:15:32:07 INFO     [evaluator_utils:446] Selected tasks:\n",
      "2026-02-26:15:32:07 INFO     [evaluator_utils:480] Task: hellaswag (hellaswag/hellaswag.yaml)\n",
      "2026-02-26:15:32:07 INFO     [api.task:312] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████████| 300/300 [00:00<00:00, 2454.85it/s]\n",
      "2026-02-26:15:32:07 INFO     [evaluator:575] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 1200/1200 [03:19<00:00,  6.01it/s]\n",
      "2026-02-26:15:35:29 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "hf-outlier ({'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True}), gen_kwargs: ({}), limit: 300.0, num_fewshot: None, batch_size: 1\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |↑  |0.5067|±  |0.0289|\n",
      "|         |       |none  |     0|acc_norm|↑  |0.6733|±  |0.0271|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} evaluate.py \\\n",
    "  --model hf-outlier \\\n",
    "  --model_args pretrained={MODEL},load_in_8bit=True \\\n",
    "  --tasks hellaswag \\\n",
    "  --device {DEVICE} \\\n",
    "  --limit {LIMIT} \\\n",
    "  --output_path {BASE_OUT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac2022-b7cc-47cd-b7cd-1b5c3d41c2f8",
   "metadata": {},
   "source": [
    "## Random Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19676e70-d396-4685-9262-afac56c183c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-26:15:46:39 WARNING  [config.evaluate_config:281] --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2026-02-26:15:46:45 INFO     [_cli.run:378] Selected Tasks: ['hellaswag']\n",
      "2026-02-26:15:46:45 INFO     [evaluator:213] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2026-02-26:15:46:45 INFO     [evaluator:238] Initializing hf-outlier model, with arguments: {'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True, 'outlier_method': 'random_ablate_1234'}\n",
      "2026-02-26:15:46:45 INFO     [models.huggingface:161] Using device 'cuda:0'\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.26s/it]\n",
      "Random single weight ablation\n",
      "[RANDOM ABLATION] layer=2 idx=(2863,3795) old=-13 new=0.0\n",
      "Not restoring or scaling GO...\n",
      "2026-02-26:15:46:56 INFO     [evaluator_utils:446] Selected tasks:\n",
      "2026-02-26:15:46:56 INFO     [evaluator_utils:480] Task: hellaswag (hellaswag/hellaswag.yaml)\n",
      "2026-02-26:15:46:56 INFO     [api.task:312] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████████| 300/300 [00:00<00:00, 2267.73it/s]\n",
      "2026-02-26:15:46:57 INFO     [evaluator:575] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████| 1200/1200 [03:20<00:00,  5.98it/s]\n",
      "2026-02-26:15:50:19 INFO     [loggers.evaluation_tracker:247] Saving results aggregated\n",
      "hf-outlier ({'pretrained': 'huggyllama/llama-7B', 'load_in_8bit': True, 'outlier_method': 'random_ablate_1234'}), gen_kwargs: ({}), limit: 300.0, num_fewshot: None, batch_size: 1\n",
      "|  Tasks  |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\n",
      "|---------|------:|------|-----:|--------|---|-----:|---|-----:|\n",
      "|hellaswag|      1|none  |     0|acc     |↑  |0.5067|±  |0.0289|\n",
      "|         |       |none  |     0|acc_norm|↑  |0.6667|±  |0.0273|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} evaluate.py \\\n",
    "  --model hf-outlier \\\n",
    "  --model_args pretrained={MODEL},load_in_8bit=True,outlier_method=random_ablate_1234 \\\n",
    "  --tasks hellaswag \\\n",
    "  --device {DEVICE} \\\n",
    "  --limit {LIMIT} \\\n",
    "  --output_path {RAND_OUT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205392b-3e36-4b1d-a07f-27f1e836eb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMSuperWeight venv)",
   "language": "python",
   "name": "llmsw-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
